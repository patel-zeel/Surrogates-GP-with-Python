{
  
    
        "post0": {
            "title": "Chapter 2",
            "content": "",
            "url": "https://patel-zeel.github.io/Surrogates-GP-with-Python/gp/2021/02/20/Chap-2.html",
            "relUrl": "/gp/2021/02/20/Chap-2.html",
            "date": " • Feb 20, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Chapter 1",
            "content": "Content . import plotly.express as px import matplotlib.pyplot as plt from matplotlib.animation import FuncAnimation from matplotlib import rc import pandas as pd import numpy as np import scipy rc(&#39;font&#39;,size=12) . Simple order 1 polynomial: $ eta = 50 + 8x_1 + 3x_2$ . def first_order(x1, x2): return 50 + 8*x1 + 3*x2 fig, ax = plt.subplots() x1 = x2 = np.linspace(-1,1,100) X1, X2 = np.meshgrid(x1, x2) z = first_order(X1, X2) cntr = plt.contour(X1, X2, z, levels=20);plt.xlabel(&#39;X1&#39;);plt.ylabel(&#39;X2&#39;); plt.xticks([-1,-0.5,0,0.5,1]);plt.yticks([-1,-0.5,0,0.5,1]) ax.clabel(cntr) plt.colorbar(); . Adding interaction term: $ eta = 50 + 8x_1 + 3x_2 - 4x_1x_2$ . def first_order_i(x1, x2, c0=50, c1=8, c2=3, c3=4): return c0 + c1*x1 + c2*x2 - c3*x1*x2 x1 = x2 = np.linspace(-1,1,100) X1, X2 = np.meshgrid(x1, x2) fig, ax = plt.subplots() def update(c3): ax.cla() z = first_order_i(X1, X2, c3=c3) cntr = ax.contour(X1, X2, z, levels=20); ax.set_xlabel(&#39;X1&#39;);ax.set_ylabel(&#39;X2&#39;); ax.set_xticks([-1,-0.5,0,0.5,1]);ax.set_yticks([-1,-0.5,0,0.5,1]) ax.clabel(cntr); ax.set_title(&#39;c3 = &#39;+str(c3)) anim = FuncAnimation(fig, update, frames=np.arange(-4,5)) plt.close() rc(&#39;animation&#39;,html=&#39;jshtml&#39;) anim . &lt;/input&gt; Once Loop Reflect Adding squared terms: $ eta = 50 + 8x_1 + 3x_2 - 7x_1^2 - 3x_2^2 - 4x_1x_2$ . def simple_max(x1, x2, c0=50, c1=8, c2=3, c3=7, c4=3, c5=4): return c0 + c1*x1 + c2*x2 - c3*np.square(x1) - c4*np.square(x2) - c5*x1*x2 x1 = x2 = np.linspace(-1,1,100) X1, X2 = np.meshgrid(x1, x2) fig, ax = plt.subplots() def update(c3): ax.cla() z = simple_max(X1, X2, c3=c3) cntr = ax.contour(X1, X2, z, levels=20); ax.set_xlabel(&#39;X1&#39;);ax.set_ylabel(&#39;X2&#39;); ax.set_xticks([-1,-0.5,0,0.5,1]);ax.set_yticks([-1,-0.5,0,0.5,1]) ax.clabel(cntr); ax.set_title(&#39;c3 = &#39;+str(c3)) anim = FuncAnimation(fig, update, frames=np.arange(7-3,7+4)) plt.close() rc(&#39;animation&#39;,html=&#39;jshtml&#39;) anim . &lt;/input&gt; Once Loop Reflect Stationary ridge: $ eta = 80+4x_1+8x_2-3x_1^2-12x_2^2-12x_1x_2$ . def stat_ridge(x1, x2, c1=80, c2=4, c3=8, c4=3, c5=12, c6=12): return c1 + c2*x1 + c3*x2 - c4*np.square(x1) - c5*np.square(x2) - c6*x1*x2 x1 = x2 = np.linspace(-1,1,100) X1, X2 = np.meshgrid(x1, x2) fig, ax = plt.subplots() def update(c2): ax.cla() z = stat_ridge(X1, X2, c2=c2) cntr = ax.contour(X1, X2, z, levels=20); ax.set_xlabel(&#39;X1&#39;);ax.set_ylabel(&#39;X2&#39;); ax.set_xticks([-1,-0.5,0,0.5,1]);ax.set_yticks([-1,-0.5,0,0.5,1]) ax.clabel(cntr); ax.set_title(&#39;c2 = &#39;+str(c2)) anim = FuncAnimation(fig, update, frames=np.arange(-4,9)) plt.close() rc(&#39;animation&#39;,html=&#39;jshtml&#39;) anim . &lt;/input&gt; Once Loop Reflect Saddle point: $ eta = 80 + 4x_1 + 8x_2 - 2x_1 - 12x_2 - 12x_1x_2$ . def saddle(x1, x2, c0=80, c1=4, c2=8, c3=2, c4=12, c5=12): return c0 + c1*x1 + c2*x2 - c3*x1 - c4*x2 - c5*x1*x2 fig, ax = plt.subplots() x1 = x2 = np.linspace(-1,1,100) X1, X2 = np.meshgrid(x1, x2) z = saddle(X1, X2) cntr = plt.contour(X1, X2, z, levels=20);plt.xlabel(&#39;X1&#39;);plt.ylabel(&#39;X2&#39;); plt.xticks([-1,-0.5,0,0.5,1]);plt.yticks([-1,-0.5,0,0.5,1]) ax.clabel(cntr); . Aircraft wing weight example (1.2.1) . The weight of aircraft is considered a function of 9 parameters given by following equation, $$ W = 0.036 S_{ mathrm{w}}^{0.758} W_{ mathrm{fw}}^{0.0035} left( frac{A}{ cos^2 Lambda} right)^{0.6} q^{0.006} lambda^{0.04} left( frac{100 R_{ mathrm{tc}}}{ cos Lambda} right)^{-0.3} (N_{ mathrm{z}} W_{ mathrm{dg}})^{0.49} $$ . features = [&#39;Sw&#39;, &#39;Wfw&#39;, &#39;A&#39;, &#39;L&#39;, &#39;q&#39;, &#39;l&#39;, &#39;Rtc&#39;, &#39;Nz&#39;, &#39;Wdg&#39;] features_with_interaction = list(features) for f1_i in range(len(features)): for f2_i in range(f1_i+1, len(features)): features_with_interaction.append(features[f1_i]+&#39; &#39;+features[f2_i]) features_with_interaction = np.array(features_with_interaction) def wingwt(Sw=0.48, Wfw=0.4, A=0.38, L=0.5, q=0.62, l=0.344, Rtc=0.4, Nz=0.37, Wdg=0.38): ## put coded inputs back on natural scale Sw = Sw*(200 - 150) + 150 Wfw = Wfw*(300 - 220) + 220 A = A*(10 - 6) + 6 L = (L*(10 - (-10)) - 10) * np.pi/180 q = q*(45 - 16) + 16 l = l*(1 - 0.5) + 0.5 Rtc = Rtc*(0.18 - 0.08) + 0.08 Nz = Nz*(6 - 2.5) + 2.5 Wdg = Wdg*(2500 - 1700) + 1700 ## calculation on natural scale W = 0.036*Sw**0.758 * Wfw**0.0035 * (A/np.cos(L)**2)**0.6 * q**0.006 W = W * l**0.04 * (100*Rtc/np.cos(L))**(-0.3) * (Nz*Wdg)**(0.49) return(W) . Generating $100 times 100$ grid and checking interactions between $A$ (aspect ratio) and $N_z$ (ultimate load factor) keeping other parameters default. . x1 = x2 = np.linspace(0,1,100) A, Nz = np.meshgrid(x1, x2) wt = wingwt(A=A, Nz=Nz) fig, ax = plt.subplots() cntr = plt.contour(A, Nz, wt, levels=20);plt.xlabel(&#39;A&#39;);plt.ylabel(&#39;Nz&#39;); ticks = [0,0.2,0.4,0.6,0.8,1] plt.xticks(ticks);plt.yticks(ticks) ax.clabel(cntr); . Now, checking interactions between $ lambda$ (taper ratio) and $W_{fw}$ (weight of fuel in wing) in the same way. . x1 = x2 = np.linspace(0,1,100) l, Wfw = np.meshgrid(x1, x2) wt = wingwt(l=l, Wfw=Wfw) fig, ax = plt.subplots() cntr = plt.contour(l, Wfw, wt, levels=20);plt.xlabel(&#39;l&#39;);plt.ylabel(&#39;Wfw&#39;); ticks = [0,0.2,0.4,0.6,0.8,1] plt.xticks(ticks);plt.yticks(ticks) ax.clabel(cntr); . This interaction does not add a lot value in estimation of weight. . Generating grids for each pair (total $^9C_2$ = 36) and evaluating $100 times 100$ grid ($360K$ points) is not computationally tengible. We will use surrogate GP to learn the underlying phenomena using far lesser points. . Let&#39;s generate 1000 Latin Hypercube samples (LHS) for 9 dimensions of interest. . import pyDOE2 . X = pyDOE2.doe_lhs.lhs(9, 1000, random_state=42) plt.scatter(X[:,0], X[:,1], s=10); . Now, We will evaluate this input space in wingwt function to generate 1000 values for response variable weight. . Y = wingwt(*[X[:,i] for i in range(9)]) Y.shape . (1000,) . First we will try with 1st order polynomial regression with interaction terms. Let us use backward step selection, BIC criterion and 5 fold cv to select the best model. We will apply log transform over response variable y. . from sklearn.linear_model import LinearRegression from sklearn.preprocessing import PolynomialFeatures from sklearn.metrics import mean_squared_error from mlxtend.feature_selection import SequentialFeatureSelector as SFS trans = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False) sfs = SFS(estimator=LinearRegression(),k_features=1,forward=False,cv=0,n_jobs=-1,scoring=&#39;r2&#39;) X_trans = trans.fit_transform(X) Y_log = np.log(Y) sfs.fit(X_trans, Y_log); . Let&#39;s calculate BIC at each step and select best feature set. . def BIC(y_true, y_pred, k): # k - number of design variables RSS = np.square(y_true - y_pred).sum() n = y_true.shape[0] # Number of data points return k*np.log(n) + n*np.log(RSS/n) bic = [] f_subset = [] for f_num in range(1, len(features_with_interaction)): model = LinearRegression() f_idx = list(sfs.subsets_[f_num][&#39;feature_idx&#39;]) X_curr = X_trans[:, f_idx] model.fit(X_curr, Y_log) y_pred = model.predict(X_curr) bic.append(BIC(Y_log, y_pred, f_num)) f_subset.append(f_idx) plt.plot(bic); . best_f_subset = f_subset[np.argmin(bic)] print(&#39;Best subsets of features are&#39;, features_with_interaction[best_f_subset]) . Best subsets of features are [&#39;Sw&#39; &#39;A&#39; &#39;l&#39; &#39;Rtc&#39; &#39;Nz&#39; &#39;Wdg&#39; &#39;q Rtc&#39;] . model = LinearRegression() model.fit(X_trans[:, best_f_subset], Y_log) list(zip(features_with_interaction[best_f_subset], model.coef_)) . [(&#39;Sw&#39;, 0.21839627523361926), (&#39;A&#39;, 0.304927876180478), (&#39;l&#39;, 0.027836184996008453), (&#39;Rtc&#39;, -0.24454080966982894), (&#39;Nz&#39;, 0.41844378205613064), (&#39;Wdg&#39;, 0.19321447213716264), (&#39;q Rtc&#39;, 0.011651847094927881)] . Interaction between A and Nz is less likely to be captured though we know it exists. . Now, We will fit a GP. . import GPy . GP = GPy.models.GPRegression(X, Y.reshape(-1,1), GPy.kern.RBF(input_dim=9, active_dims=list(range(9)), ARD=True)) GP.optimize() . /home/patel_zeel/anaconda3/lib/python3.8/site-packages/GPy/kern/src/stationary.py:137: RuntimeWarning:overflow encountered in square /home/patel_zeel/anaconda3/lib/python3.8/site-packages/GPy/kern/src/stationary.py:138: RuntimeWarning:invalid value encountered in add . &lt;paramz.optimization.optimization.opt_lbfgsb at 0x7f9ba0482340&gt; . Creating a $100 times 100$ grid of A and Nz and setting other parameters to baseline. . params = [] for param in range(9): param_val = wingwt.__defaults__[param]*np.ones((10000,1)) params.append(param_val) # Modify A and Nz at positions 2 and 7 params[2] = A.reshape(-1,1) params[7] = Nz.reshape(-1,1) # Create test grid XX = np.hstack(params) XX.shape . (10000, 9) . pred_Y, pred_Var = GP.predict(XX) . fig, ax = plt.subplots(1,2, sharex=True, sharey=True,figsize=(10,4)) cntr1 = ax[0].contour(A, Nz, pred_Y.reshape(100,100), levels=20); ax[0].clabel(cntr1); wt = wingwt(A=A, Nz=Nz) cntr2 = ax[1].contour(A, Nz, wt, levels=20); ax[1].clabel(cntr2); ticks = [0,0.2,0.4,0.6,0.8,1] for i in range(2): ax[i].set_xlabel(&#39;A&#39;);ax[i].set_ylabel(&#39;Nz&#39;); ax[i].set_xticks(ticks);ax[i].set_yticks(ticks) ax[0].set_title(&#39;Predicted response&#39;) ax[1].set_title(&#39;Baseline response&#39;); . We have successfully captured the relationship between $N_z, A$ and $W$ with 1000 simulated points. . Now, doing 1D sensitivity analysis for all 9 process variables . preds = [] for i in range(9): print(i,end=&#39;&#39;) params = [] for param in range(9): param_val = wingwt.__defaults__[param]*np.ones((1000,1)) params.append(param_val) # Modify ith parameter params[i] = np.linspace(0,1,1000).reshape(-1,1) # Create test grid XX = np.hstack(params) preds.append(GP.predict(XX)[0]) print(&#39; Done&#39;) # wingwt . 012345678 Done . plt.figure(figsize=(15,5)) param_names = [&#39;Sw&#39;,&#39;Wfw&#39;,&#39;A&#39;,&#39;L&#39;,&#39;q&#39;,&#39;l&#39;,&#39;Rtc&#39;,&#39;Nz&#39;,&#39;Wdg&#39;] for i in range(9): plt.plot(np.linspace(0,1,1000), preds[i], label=param_names[i]) plt.legend(bbox_to_anchor=[1,1]); plt.xlabel(&#39;Process variable&#39;);plt.ylabel(&#39;Response variable&#39;); . Variables l, L, Wfw and q are not very useful in determining yields of response variable. . Homework . 1: Regression . The file wire.csv contains data relating the pull strength (pstren) of a wire bond (which we’ll treat as a response) to six characteristics which we shall treat as design variables: die height (dieh), post height (posth), loop height (looph), wire length (wlen), bond width on the die (diew), and bond width on the post (postw). (Derived from exercise 2.3 in Myers, Montgomery, and Anderson–Cook (2016) using data from Table E2.1.) . Write code that converts natural variables in the file to coded variables in the unit hypercube. Also, normalize responses to have a mean of zero and a range of 1. Use model selection techniques to select a parsimonious linear model for the coded data including, potentially, second-order and interaction effects. Use the fitted model to make a prediction for pull strength, when the explanatory variables take on the values c(6, 20, 30, 90, 2, 2), in the order above, with a full accounting of uncertainty. Make sure the predictive quantities are on the original scale of the data. . from sklearn.preprocessing import StandardScaler . wire_data = pd.read_csv(&#39;data/wire.csv&#39;) wire_data.head(1) . pstren dieh posth looph wlen diew postw . 0 8.0 | 5.2 | 19.6 | 29.6 | 94.9 | 2.1 | 2.3 | . Xscaler = StandardScaler() trans = PolynomialFeatures(degree=2, include_bias=False) X_trans = trans.fit_transform(wire_data.drop(&#39;pstren&#39;,axis=1)) X_scaled = Xscaler.fit_transform(X_trans) Yscaler = StandardScaler() y_scaled = Yscaler.fit_transform(wire_data[[&#39;pstren&#39;]]) . sfs = SFS(estimator=LinearRegression(), k_features=1, forward=False, cv=0, n_jobs=-1, scoring=&#39;r2&#39;) sfs.fit(X_scaled, y_scaled.squeeze()); . bic = [] f_subset = [] for f_num in range(1, X_scaled.shape[1]): model = LinearRegression() f_idx = list(sfs.subsets_[f_num][&#39;feature_idx&#39;]) X_curr = X_scaled[:, f_idx] model.fit(X_curr, y_scaled) y_pred = model.predict(X_curr) bic.append(BIC(y_scaled, y_pred, f_num)) f_subset.append(f_idx) plt.plot(bic); . Model selection is happening after including many features. Let&#39;s keep the best model. . best_f_subset = f_subset[np.argmin(bic)] . model = LinearRegression() model.fit(X_scaled[:, best_f_subset], y_scaled); . Now, we will predict at c(6, 20, 30, 90, 2, 2) . X_new = np.array([6, 20, 30, 90, 2, 2]).reshape(1,-1) X_new_trans = trans.transform(X_new) X_new_scaled = Xscaler.transform(X_new_trans) X_new_select = X_new_scaled[:, best_f_subset] print(&#39;pstren is&#39;, Yscaler.inverse_transform(model.predict(X_new_select))) . pstren is [[10.68053283]] . 2: Surrogates for sensitivity . Consider the so-called piston simulation function which was at one time a popular benchmark problem in the computer experiments literature. (That link, to Surjanovic and Bingham (2013)’s Virtual Library of Simulation Experiments (VLSE), provides references and further detail. VLSE is a great resource for test functions and computer simulation experiment data; there’s a page for the wing weight example as well.) Response $C(x)$ is the cycle time, in seconds, of the circular motion of a piston within a gas-filled cylinder, the configuration of which is described by seven-dimensional input vector $x=(M,S,V_0,k,P_0,T_a,T_0)$. $$ begin{aligned} C(x) = 2 pi sqrt{ frac{M}{k + S^2 frac{P_0 V_0}{T_0} frac{T_a}{V^2}}}, quad mbox{where } V &amp;= frac{S}{2k} left( sqrt{A^2 + 4k frac{P_0 V_0}{T_0} T_a} - A right) mbox{and } A &amp;= P_0 S + 19.62 M - frac{k V_0}{S} end{aligned} $$ . Table 1.2 describes the input coordinates of x, their ranges, and provides a baseline value derived from the middle of the specified range(s). . Explore $C(x)$ with techniques similar to those used on the wing weight example (§1.2.1). Start with a space-filling (LHS) design in 7d and fit a GP surrogate to the responses. Use predictive equations to explore main effects and interactions between pairs of inputs. In your solution, rather than showing all $^7C_2=21$ pairs, select one “interesting” and another “uninteresting” one and focus your presentation on those two. How do your surrogate predictions for those pairs compare to an exhaustive 2d grid-based evaluation and visualization of $C(x)$? . features = [&#39;M&#39;, &#39;S&#39;, &#39;V0&#39;, &#39;k&#39;, &#39;P0&#39;, &#39;Ta&#39;, &#39;T0&#39;] def piston(M=0.5, S=0.5, V0=0.5, k=0.5, P0=0.5, Ta=0.5, T0=0.5): ## put coded inputs back on natural scale M = M*(60 - 30) + 30 S = S*(0.02 - 0.005) + 0.005 V0 = V0*(0.01 - 0.002) + 0.002 k = k*(5000 - 1000) + 1000 P0 = P0*(110000 - 90000) + 90000 Ta = Ta*(296 - 290) + 290 T0 = T0*(360 - 340) + 340 A = P0*S + 19.62*M - k*V0/S Vfact1 = S/(2*k) Vfact2 = np.sqrt(np.square(A) + 4*k*(P0*V0/T0)*Ta) V = Vfact1 * (Vfact2 - A) fact1 = M fact2 = k + (np.square(S))*(P0*V0/T0)*(Ta/(np.square(V))) C = 2 * np.pi * np.sqrt(fact1/fact2) return(C) . Let&#39;s generate 1000 number of 7 dimensional LHS samples. . X_pist = pyDOE2.doe_lhs.lhs(7, samples=1000) Y_pist = piston(*[X_pist[:,i] for i in range(X_pist.shape[1])]) X_pist.shape, Y_pist.shape . ((1000, 7), (1000,)) . Now, we will fit a GP surrogate model to this data. . model = GPy.models.GPRegression(X_pist, Y_pist.reshape(-1,1), GPy.kern.RBF(input_dim=X_pist.shape[1], active_dims=list(range(X_pist.shape[1])), ARD=True)) model.optimize() . &lt;paramz.optimization.optimization.opt_lbfgsb at 0x7f9ba0493d30&gt; . Now we will do 1D sensitivity analysis and check how ground truth matches with surrogate. . preds = [] trues = [] for i in range(7): print(i,end=&#39;&#39;) params = [] for param in range(7): param_val = piston.__defaults__[param]*np.ones((1000,1)) params.append(param_val) # Modify ith parameter params[i] = np.linspace(0,1,1000).reshape(-1,1) # Create test grid XX = np.hstack(params) preds.append(model.predict(XX)[0]) trues.append(piston(*[XX[:,i] for i in range(7)])) print(&#39; Done&#39;) . 0123456 Done . fig, ax = plt.subplots(1,2,figsize=(16,5), sharex=True, sharey=True) param_names = [&#39;M&#39;,&#39;S&#39;,&#39;V0&#39;,&#39;k&#39;,&#39;P0&#39;,&#39;Ta&#39;,&#39;T0&#39;] linestyles = [&#39;--&#39;,&#39;-&#39;] for i in range(7): ax[0].plot(np.linspace(0,1,1000), preds[i], linestyles[i%2],label=param_names[i]) ax[1].plot(np.linspace(0,1,1000), trues[i], linestyles[i%2],label=param_names[i]) ax[0].set_xlabel(&#39;Process variable&#39;);ax[1].set_xlabel(&#39;Process variable&#39;); ax[0].set_title(&#39;Main effects - Surrogate&#39;);ax[1].set_title(&#39;Main effects - Ground truth&#39;) ax[0].set_ylabel(&#39;Cycle speed (in seconds)&#39;); ax[1].legend(bbox_to_anchor=[1,1]); . We see that S, V0, M and k are causing major impact on cycle speed of the piston. Let&#39;s visualize relationship between S and M0 that should be &quot;interesting&quot; (as asked in question). . X_all = np.ones((10000,7))*0.5 S, V0 = np.meshgrid(np.linspace(0,1,100), np.linspace(0,1,100)) X_all[:,1] = S.ravel() X_all[:,2] = V0.ravel() C_true = piston(*[X_all[:,i] for i in range(X_all.shape[1])]) C_pred = model.predict(X_all)[0] fig, ax = plt.subplots(1,2, sharex=True, sharey=True,figsize=(10,4)) cntr1 = ax[0].contour(S, V0, C_pred.reshape(100,100), levels=20); ax[0].clabel(cntr1); cntr2 = ax[1].contour(S, V0, C_true.reshape(100,100), levels=20); ax[1].clabel(cntr2); ticks = [0,0.2,0.4,0.6,0.8,1] for i in range(2): ax[i].set_xlabel(&#39;S&#39;);ax[0].set_ylabel(&#39;V0&#39;); ax[i].set_xticks(ticks);ax[i].set_yticks(ticks) ax[0].set_title(&#39;Predicted response&#39;) ax[1].set_title(&#39;Baseline response&#39;); . Now, checking relationship between Ta and P0. . X_all = np.ones((10000,7))*0.5 Ta, P0 = np.meshgrid(np.linspace(0,1,100), np.linspace(0,1,100)) X_all[:,5] = Ta.ravel() X_all[:,4] = P0.ravel() C_true = piston(*[X_all[:,i] for i in range(X_all.shape[1])]) C_pred = model.predict(X_all)[0] fig, ax = plt.subplots(1,2, sharex=True, sharey=True,figsize=(10,4)) cntr1 = ax[0].contour(Ta, P0, C_pred.reshape(100,100), levels=20); ax[0].clabel(cntr1); cntr2 = ax[1].contour(Ta, P0, C_true.reshape(100,100), levels=20); ax[1].clabel(cntr2); ticks = [0,0.2,0.4,0.6,0.8,1] for i in range(2): ax[i].set_xlabel(&#39;Ta&#39;);ax[0].set_ylabel(&#39;P0&#39;); ax[i].set_xticks(ticks);ax[i].set_yticks(ticks) ax[0].set_title(&#39;Predicted response&#39;) ax[1].set_title(&#39;Baseline response&#39;); . As we can see the relationship is &quot;uninteresting&quot; both marginally and jointly. . 3: Optimization . Consider two-dimensional functions $f$ and $c$ , defined over $[ 0 , 1 ]^2$ ; $f$ is a re-scaled version of the so-called Goldstein–Price function, and is defined in terms of auxiliary functions $a$ and $b$. $$ begin{aligned} f(x) &amp;= frac{ log left[(1+a(x)) (30 + b(x)) right] - 8.69}{2.43} quad mbox{with} a(x) &amp;= left(4 x_1 + 4 x_2 - 3 right)^2 &amp; ; ; ; times left[ 75 - 56 left(x_1 + x_2 right) + 3 left(4 x_1 - 2 right)^2 + 6 left(4 x_1 - 2 right) left(4 x_2 - 2 right) + 3 left(4 x_2 - 2 right)^2 right] b(x) &amp;= left(8 x_1 - 12 x_2 +2 right)^2 &amp; ; ; ; times left[-14 - 128 x_1 + 12 left(4 x_1 - 2 right)^2 + 192 x_2 - 36 left(4 x_1 - 2 right) left(4 x_2 - 2 right) + 27 left(4 x_2 - 2 right)^2 right] end{aligned} $$ Separately, let a “constraint” function $c$ be defined as, $$ c(x) = frac{3}{2} - x_1 - 2x_2 - frac{1}{2} sin(2 pi(x_1^2 - 2x_2)) $$ . Evaluate $f$ on a grid and make an image and/or image–contour plot of the surface. | Use a library routine (e.g., optim in R) to find the global minimum. When optimizing, pretend you don’t know the form of the function; i.e., treat it as a “blackbox”. Initialize your search randomly and comment on the behavior over repeated random restarts. About how many evaluations does it take to find the local optimum in each initialization repeat; about how many to reliably find the global one across repeats? | Now, re-create your plot from #a with contours only (no image), and then add color to the plot indicating the region(s) where $c(x)&gt;0$ and $c(x)≤0$ ,respectively. To keep it simple, choose white for the latter, say. | Use a library routine (e.g., nloptr in R) to solve the following constrained optimization problem: min $f(x)$ such that $c(x)≤0$ and $x ∈ [ 0 , 1 ]^2$. Initialize your search randomly and comment on the behavior over repeated random restarts. About how many evaluations does it take to find the local valid optimum in each initialization repeat; about how many to reliably find the global one across repeats? | Ans | . First, let&#39;s create a grid and make an image of the surface of $f$. . def goldprice(xx): x1bar = 4*xx[0] - 2 x2bar = 4*xx[1] - 2 fact1a = np.square(x1bar + x2bar + 1) fact1b = 19 - 14*x1bar + 3*np.square(x1bar) - 14*x2bar + 6*x1bar*x2bar + 3*np.square(x2bar) fact1 = 1 + fact1a*fact1b fact2a = np.square(2*x1bar - 3*x2bar) fact2b = 18 - 32*x1bar + 12*np.square(x1bar) + 48*x2bar - 36*x1bar*x2bar + 27*np.square(x2bar) fact2 = 30 + fact2a*fact2b prod = fact1*fact2 y = (np.log(prod) - 8.693) / 2.427 return y def c(xx): x1, x2 = xx return 1.5 - x1 - 2*x2 - 0.5*np.sin(2*np.pi*(np.square(x1)-2*x2)) . x1, x2 = np.meshgrid(np.linspace(0,1,100), np.linspace(0,1,100)) gprice = goldprice([x1, x2]) cisneg = c([x1, x2])&lt;=0 fig, ax = plt.subplots(figsize=(15,6)) cntr = plt.contour(x1, x2, gprice, levels=20);plt.xlabel(&#39;x1&#39;);plt.ylabel(&#39;x2&#39;); cntr2 = plt.contourf(x1, x2, cisneg, alpha=0.4) ax.clabel(cntr) plt.colorbar(); plt.title(&#39;goldstain function shown by contour lines. Yellow shade is $c(x) &lt;= 0$&#39;); . Optimizing without constaints . optims = [] N = 200 for seed in range(N): np.random.seed(seed) res = scipy.optimize.minimize(goldprice, [np.abs(np.random.rand()),np.abs(np.random.rand())]) optims.append(res.fun) plt.figure(figsize=(15,4)) plt.xticks(list(range(N))[::10]) plt.scatter(range(N),optims);plt.xlabel(&#39;Seed&#39;);plt.ylabel(&#39;Optimal function value&#39;); goptim = np.min(optims) print(&#39;Out of&#39;,N,&#39;restarts&#39;,(np.array(optims)&lt;goptim+0.01).sum(),&#39;found global minima&#39;,goptim) . Out of 200 restarts 104 found global minima -3.129125550610589 . Optimizing with constaints, $c(x) le 0$ . optims = [] for seed in range(N): np.random.seed(seed) cons = scipy.optimize.NonlinearConstraint(c, -np.inf, 0) res = scipy.optimize.minimize(goldprice, [np.abs(np.random.rand()),np.abs(np.random.rand())], constraints=cons) optims.append(res.fun) plt.figure(figsize=(15,4)) plt.xticks(list(range(N))[::10]) plt.scatter(range(N),optims);plt.xlabel(&#39;Seed&#39;);plt.ylabel(&#39;Optimal function value&#39;); goptim = np.min(optims) plt.ylim(-3,4); print(&#39;Out of&#39;,N,&#39;restarts&#39;,(np.array(optims)&lt;goptim+0.01).sum(),&#39;found global minima&#39;,goptim) . Out of 200 restarts 70 found global minima -2.180388388271195 . Optimizing with constaints, $c(x) &gt; 0$ . optims = [] for seed in range(N): np.random.seed(seed) cons = scipy.optimize.NonlinearConstraint(c, 0, np.inf) res = scipy.optimize.minimize(goldprice, [np.abs(np.random.rand()),np.abs(np.random.rand())], constraints=cons) optims.append(res.fun) plt.figure(figsize=(15,4)) plt.xticks(list(range(N))[::10]) plt.scatter(range(N),optims);plt.xlabel(&#39;Seed&#39;);plt.ylabel(&#39;Optimal function value&#39;); goptim = np.min(optims) plt.ylim(-4,4); print(&#39;Out of&#39;,N,&#39;restarts&#39;,(np.array(optims)&lt;goptim+0.01).sum(),&#39;found global minima&#39;,goptim) . Out of 200 restarts 118 found global minima -3.1291255506097215 .",
            "url": "https://patel-zeel.github.io/Surrogates-GP-with-Python/gp/2021/02/20/Chap-1.html",
            "relUrl": "/gp/2021/02/20/Chap-1.html",
            "date": " • Feb 20, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://patel-zeel.github.io/Surrogates-GP-with-Python/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://patel-zeel.github.io/Surrogates-GP-with-Python/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}