{
  
    
        "post0": {
            "title": "Chapter 2",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.tri as tri from polire.interpolate import Kriging . Rocket booster dynamics . NASA explored possibility of creating a launcher that can be reused after launching the payload. Simulation was created for this and below case study explores the two experiments performed on emulators and story behind that. . Data has three input variables (mach, alpha and beta) and six output variables. This is taken from the initial experiments containing 3167 simulations. . lgbb1 = pd.read_table(&#39;data/lgbb/lgbb_original.txt&#39;, sep=&#39; &#39;, skiprows=21) print(&#39;shape of data is&#39;,lgbb1.shape) lgbb1.head(2) . shape of data is (3167, 9) . mach alpha beta lift drag pitch side yaw roll . 0 0.2 | -5 | 0.0 | -0.09536 | 0.00060 | -0.02869 | 0.00076 | -0.00032 | 0.00010 | . 1 0.2 | -2 | 0.0 | 0.05032 | 0.02859 | -0.03162 | 0.00139 | -0.00045 | 0.00016 | . Let&#39;s see relation between alpha, mach and lift. . interp = Kriging() interp.fit(lgbb1[[&#39;mach&#39;,&#39;alpha&#39;]].values, lgbb1[&#39;lift&#39;].values) Xi, Yi = np.meshgrid(np.linspace(0,6,200), np.linspace(-5,30,200)) XiYi = np.vstack([Xi.ravel(),Yi.ravel()]).T zi = interp.predict(XiYi).reshape(200,200) plt.scatter(lgbb1.mach, lgbb1.alpha,s=6,c=lgbb1.lift); cntr = plt.contourf(Xi, Yi, zi, alpha=0.6); plt.colorbar();plt.xlabel(&#39;Mach&#39;);plt.ylabel(&#39;alpha&#39;); plt.title(&#39;Effect on lift due to mach and alpha&#39;); . Sampling is dense at region where alpha is high and mach ~ 1. This was done to accomodate high variations at that region. . Grid has two drawbacks here i) Despite high number of samples, unique values per input variables are less; ii) Numerical stability. . Let&#39;s visualize relation between mach and lift when beta is zero and non-zero (keeping alpha==1). . tmp_df = lgbb1.sort_values(&#39;mach&#39;) tmp_df = tmp_df[tmp_df[&#39;alpha&#39;]==1] fig, ax = plt.subplots() tmp_df[tmp_df[&#39;beta&#39;] != 0].plot(x=&#39;mach&#39;,y=&#39;lift&#39;,ax=ax, label=&#39;beta != 0&#39;,ylabel=&#39;lift&#39;); tmp_df[tmp_df[&#39;beta&#39;] == 0].plot(x=&#39;mach&#39;,y=&#39;lift&#39;,ax=ax, label=&#39;beta == 0&#39;); . When beta != 0, lift output has problems here and also this plot has low resolution. So, second experiment was carried out to solve following problems. . an adaptive design without gridding | better numerics (improvements to Cart3D) | paired with an ability to back-out a high resolution surface, smoothing out the gaps, based on relatively few total simulations | . To generate second version of dataset, active learning with treed gaussian process (TGP) model are used. This time only 780 simulations were done. . lgbb2 = pd.read_table(&#39;data/lgbb/lgbb_as.txt&#39;,sep=&#39; &#39;, skiprows=32) lgbb2 = lgbb2.set_index(&#39;index&#39;).sort_index() . lgbb2.plot(x=&#39;mach&#39;,y=&#39;alpha&#39;,kind=&#39;scatter&#39;); . Values chosen for mach variable has become much finer compared to earlier experiments. . for col in [&#39;mach&#39;,&#39;alpha&#39;,&#39;beta&#39;]: print(&#39;lgbb1 unique values for&#39;,col,&#39;=&#39;,lgbb1[col].unique().shape[0]) print(&#39;lgbb2 unique values for&#39;,col,&#39;=&#39;,lgbb2[col].unique().shape[0]) print() . lgbb1 unique values for mach = 37 lgbb2 unique values for mach = 110 lgbb1 unique values for alpha = 33 lgbb2 unique values for alpha = 36 lgbb1 unique values for beta = 6 lgbb2 unique values for beta = 9 . Now, plotting the beta==0 and beta != 0 (alpha == 1)figure again. . tmp_df = lgbb2.sort_values(&#39;mach&#39;) tmp_df = tmp_df[tmp_df[&#39;alpha&#39;]==1] fig, ax = plt.subplots() tmp_df[tmp_df[&#39;beta&#39;] != 0].plot(x=&#39;mach&#39;,y=&#39;lift&#39;,ax=ax, label=&#39;beta != 0&#39;,ylabel=&#39;lift&#39;); tmp_df[tmp_df[&#39;beta&#39;] == 0].plot(x=&#39;mach&#39;,y=&#39;lift&#39;,ax=ax, label=&#39;beta == 0&#39;,kind=&#39;scatter&#39;); . We see that peak is removed now. . Visualizing interpolated dense surface from TGP results. . import rdata parsed = rdata.parser.parse_file(&#39;data/lgbb/lgbb_fill.RData&#39;) lgbb_fill = rdata.conversion.convert(parsed)[&#39;lgbb.fill&#39;] . tmp_df = lgbb_fill[lgbb_fill.beta==1] interp = Kriging() interp.fit(tmp_df[[&#39;mach&#39;,&#39;alpha&#39;]].values, tmp_df[&#39;lift&#39;].values) Xi, Yi = np.meshgrid(np.linspace(0,6,200), np.linspace(-5,30,200)) XiYi = np.vstack([Xi.ravel(),Yi.ravel()]).T zi = interp.predict(XiYi).reshape(200,200) plt.scatter(tmp_df.mach, tmp_df.alpha,s=6,c=tmp_df.lift); cntr = plt.contourf(Xi, Yi, zi, alpha=0.6); plt.colorbar();plt.xlabel(&#39;Mach&#39;);plt.ylabel(&#39;alpha&#39;); plt.title(&#39;Effect on lift due to mach and alpha (beta==1)&#39;); . The plot suggests complex relationship near mach==1 and high alpha values. . Checking relationship between mach and lift for all unique values of alpha (beta==1). . fig, ax = plt.subplots() for i,val in enumerate(lgbb_fill.alpha.unique()): tmp_df = lgbb_fill[lgbb_fill.alpha==val] tmp_df = tmp_df[tmp_df.beta==1] plt.plot(tmp_df[&#39;mach&#39;], tmp_df[&#39;lift&#39;], label=&#39;alpha=&#39;+str(val.round(2))); plt.legend(bbox_to_anchor=[1,1]); plt.xlabel(&#39;mach&#39;);plt.ylabel(&#39;lift&#39;); . The key of this experiment was to understand dynamics better. Experiments found interesting region automatically with Active Learning techniques. In this case, design was simple to sample more points where uncertainty is high. . Radiative shock hydrodynamics . We are interested in knowing location of radiative show wave head in xenon tube in this experiment. It contains 9 design variables but observations are too low (20). . crash = pd.read_csv(&#39;data/crash/CRASHExpt_clean.csv&#39;) crash[&#39;BeThickness&#39;] = 21 print(&#39;shape of data is&#39;,crash.shape) crash.head(2) . shape of data is (20, 9) . LaserEnergy GasPressure AspectRatio NozzleLength TaperLength TubeDiameter Time ShockLocation BeThickness . 0 3889.6 | 1.133 | 1 | 500 | 500 | 575 | 1.400000e-08 | 2285.8 | 21 | . 1 3889.6 | 1.133 | 1 | 500 | 500 | 575 | 1.600000e-08 | 2499.9 | 21 | . In experiment 1 (ce1) laser energy scale factor is used. In 2nd experiment directly effective laser energy is used. . ce1 = pd.read_csv(&quot;data/crash/RS12_SLwithUnnormalizedInputs.csv&quot;) ce2 = pd.read_csv(&quot;data/crash/RS13Minor_SLwithUnnormalizedInputs.csv&quot;) ce2.ElectronFluxLimiter = 0.06 . sfmin = ce2.EffectiveLaserEnergy/5000 sflen = 10 ce2_sf = pd.DataFrame(np.random.rand(sflen*ce2.shape[0], ce2.shape[1]+2)*np.nan) for i in range(1,sflen+1): sfi = sfmin + (1 - sfmin)*(i/sflen) ce2_sf.iloc[(i-1)*ce2.shape[0] + np.arange(ce2.shape[0]),:] = np.hstack([ce2.values, sfi.values.reshape(-1,1), (ce2.EffectiveLaserEnergy/sfi).values.reshape(-1,1)]) ce2_sf.columns = ce2.columns.tolist() + [&quot;EnergyScaleFactor&quot;, &quot;LaserEnergy&quot;] ce2_sf.columns . Index([&#39;FileNumber&#39;, &#39;BeThickness&#39;, &#39;EffectiveLaserEnergy&#39;, &#39;GasPressure&#39;, &#39;AspectRatio&#39;, &#39;NozzleLength&#39;, &#39;TaperLength&#39;, &#39;TubeDiameter&#39;, &#39;Time&#39;, &#39;ElectronFluxLimiter&#39;, &#39;ShockLocation&#39;, &#39;EnergyScaleFactor&#39;, &#39;LaserEnergy&#39;], dtype=&#39;object&#39;) . plt.scatter(ce2_sf.LaserEnergy, ce2_sf.EnergyScaleFactor,c=&#39;w&#39;,edgecolors= &quot;black&quot;,label=&#39;ce2&#39;) plt.scatter(ce1.LaserEnergy, ce1.EnergyScaleFactor,c=&#39;r&#39;,label=&#39;ce1&#39;); plt.legend(bbox_to_anchor=[1.2,1]); . Satelite drag . tpm library developed (wrapper over original C library) by author is used to calculate satelite drag. Then a GP surrogate was used to have root mean sqaured percentage error below 1%. . Actual design had too many data points that were beyong GP capability to model. So, range of thata and phi was reduced to generate 1000 train and 100 test points using LHS. . names = [&#39;Umag&#39;,&#39;Ts&#39;,&#39;Ta&#39;,&#39;alphan&#39;,&#39;sigmat&#39;,&#39;theta&#39;,&#39;phi&#39;,&#39;Cd&#39;] train = pd.read_csv(&quot;data/lanl/GRACE/CD_GRACE_1000_He.dat&quot;,sep=&#39; &#39;,names=names) test = pd.read_csv(&quot;data/lanl/GRACE/CD_GRACE_100_He.dat&quot;,sep=&#39; &#39;,names=names) train.describe() . Umag Ts Ta alphan sigmat theta phi Cd . count 1000.000000 | 1000.000000 | 1000.000000 | 1000.000000 | 1000.000000 | 1000.000000 | 1000.000000 | 1000.000000 | . mean 7513.339621 | 299.993130 | 1100.012022 | 0.500008 | 0.500006 | 0.034134 | 0.000050 | 3.737018 | . std 1176.512980 | 115.524068 | 519.877619 | 0.288828 | 0.288817 | 0.020290 | 0.040970 | 0.828606 | . min 5501.933000 | 100.312700 | 201.223200 | 0.000882 | 0.000761 | 0.000013 | -0.069781 | 1.994381 | . 25% 6502.452500 | 200.194700 | 649.939925 | 0.250170 | 0.250216 | 0.016469 | -0.036841 | 3.117479 | . 50% 7505.025000 | 299.956950 | 1099.937500 | 0.500369 | 0.500341 | 0.034206 | -0.000410 | 3.654797 | . 75% 8568.225000 | 399.841525 | 1549.948250 | 0.749546 | 0.749722 | 0.052085 | 0.036675 | 4.269964 | . max 9497.882000 | 499.841000 | 1999.999000 | 0.999215 | 0.999790 | 0.069783 | 0.069663 | 6.789995 | . Visualizing LHS design . plt.scatter(train.Umag, train.Ts); plt.xlabel(&#39;Umag&#39;);plt.ylabel(&#39;Ts&#39;); . from sklearn.preprocessing import MinMaxScaler, StandardScaler import GPy . scaler = StandardScaler() train_scaled, test_scaled = train.copy(), test.copy() scaler.fit(pd.concat([train,test])) train_scaled[train.columns] = scaler.transform(train) test_scaled[test.columns] = scaler.transform(test) . Fitting GP model . X_train = train_scaled.values[:,:7] X_test = test_scaled.values[:,:7] y_train = train.values[:,7:8] y_test = test.values[:,7:8] . dim = X_train.shape[1] kernel = GPy.kern.RBF(dim,active_dims=range(dim),ARD=True) model = GPy.models.GPRegression(X_train, y_train, kernel) model.optimize() . &lt;paramz.optimization.optimization.opt_lbfgsb at 0x7f5d30bcae80&gt; . y_pred = model.predict(X_test)[0] rmspe = np.sqrt(np.mean(np.square(100*(y_pred-y_test)/y_test))) rmspe . 0.7587825861705741 . rmspe is less than 1% as required. . Homework . 1: The other five LGBB outputs . lgbb_fill.columns . Index([&#39;mach&#39;, &#39;alpha&#39;, &#39;beta&#39;, &#39;lift&#39;, &#39;drag&#39;, &#39;pitch&#39;, &#39;side&#39;, &#39;yaw&#39;, &#39;roll&#39;], dtype=&#39;object&#39;) . drag . tmp_df = lgbb_fill[lgbb_fill.beta==1] interp = Kriging() interp.fit(tmp_df[[&#39;mach&#39;,&#39;alpha&#39;]].values, tmp_df[&#39;drag&#39;].values) Xi, Yi = np.meshgrid(np.linspace(0,6,200), np.linspace(-5,30,200)) XiYi = np.vstack([Xi.ravel(),Yi.ravel()]).T zi = interp.predict(XiYi).reshape(200,200) plt.scatter(tmp_df.mach, tmp_df.alpha,s=6,c=tmp_df.lift); cntr = plt.contourf(Xi, Yi, zi, alpha=0.6); plt.colorbar();plt.xlabel(&#39;Mach&#39;);plt.ylabel(&#39;alpha&#39;); plt.title(&#39;Effect on drag due to mach and alpha (beta==1)&#39;); . fig, ax = plt.subplots() for i,val in enumerate(lgbb_fill.alpha.unique()): tmp_df = lgbb_fill[lgbb_fill.alpha==val] tmp_df = tmp_df[tmp_df.beta==1] plt.plot(tmp_df[&#39;mach&#39;], tmp_df[&#39;drag&#39;], label=&#39;alpha=&#39;+str(val.round(2))); plt.legend(bbox_to_anchor=[1,1]); plt.xlabel(&#39;mach&#39;);plt.ylabel(&#39;drag&#39;); . side beta ==0 and beta == 1 . tmp_dfs = [lgbb_fill[lgbb_fill.beta==1], lgbb_fill[lgbb_fill.beta==0]] beta = [&#39;1&#39;,&#39;0&#39;] fig, ax = plt.subplots(1,2,figsize=(15,4)) zis = [] for i, tmp_df in enumerate(tmp_dfs): interp = Kriging() interp.fit(tmp_df[[&#39;mach&#39;,&#39;alpha&#39;]].values, tmp_df[&#39;side&#39;].values) Xi, Yi = np.meshgrid(np.linspace(0,6,200), np.linspace(-5,30,200)) XiYi = np.vstack([Xi.ravel(),Yi.ravel()]).T zi = interp.predict(XiYi).reshape(200,200) zis.append(zi) ax[i].scatter(tmp_df.mach, tmp_df.alpha,s=6,c=tmp_df.lift); cntr = ax[i].contourf(Xi, Yi, zi, alpha=0.6); ax[i].set_xlabel(&#39;Mach&#39;);ax[i].set_ylabel(&#39;alpha&#39;); ax[i].set_title(&#39;Effect on side due to mach and alpha. beta =&#39;+beta[i]); . Now, checking side for each unique value of beta . fig, axs = plt.subplots(3,3,figsize=(15,10), sharey=True) ax = axs.ravel() for f_i,beta in enumerate(np.arange(0,4.5,0.5)): tmp_df = lgbb_fill[lgbb_fill.beta==beta] for i,val in enumerate(tmp_df.alpha.unique()): tmp_df2 = tmp_df[tmp_df.alpha==val] ax[f_i].plot(tmp_df2[&#39;mach&#39;], tmp_df2[&#39;side&#39;], label=&#39;alpha=&#39;+str(val.round(2))); # ax[beta].legend(bbox_to_anchor=[1,1]); ax[f_i].set_xlabel(&#39;mach&#39;);ax[f_i].set_ylabel(&#39;side&#39;); ax[f_i].set_title(&#39;beta = &#39;+str(beta)) plt.tight_layout() . yaw . fig, axs = plt.subplots(3,3,figsize=(15,10), sharey=True) ax = axs.ravel() for f_i,beta in enumerate(np.arange(0,4.5,0.5)): tmp_df = lgbb_fill[lgbb_fill.beta==beta] for i,val in enumerate(tmp_df.alpha.unique()): tmp_df2 = tmp_df[tmp_df.alpha==val] ax[f_i].plot(tmp_df2[&#39;mach&#39;], tmp_df2[&#39;yaw&#39;], label=&#39;alpha=&#39;+str(val.round(2))); # ax[beta].legend(bbox_to_anchor=[1,1]); ax[f_i].set_xlabel(&#39;mach&#39;);ax[f_i].set_ylabel(&#39;yaw&#39;); ax[f_i].set_title(&#39;beta = &#39;+str(beta)) plt.tight_layout() . roll . fig, axs = plt.subplots(3,3,figsize=(15,10), sharey=True) ax = axs.ravel() for f_i,beta in enumerate(np.arange(0,4.5,0.5)): tmp_df = lgbb_fill[lgbb_fill.beta==beta] for i,val in enumerate(tmp_df.alpha.unique()): tmp_df2 = tmp_df[tmp_df.alpha==val] ax[f_i].plot(tmp_df2[&#39;mach&#39;], tmp_df2[&#39;roll&#39;], label=&#39;alpha=&#39;+str(val.round(2))); # ax[beta].legend(bbox_to_anchor=[1,1]); ax[f_i].set_xlabel(&#39;mach&#39;);ax[f_i].set_ylabel(&#39;roll&#39;); ax[f_i].set_title(&#39;beta = &#39;+str(beta)) plt.tight_layout() . lift . fig, axs = plt.subplots(3,3,figsize=(15,10), sharey=True) ax = axs.ravel() for f_i,beta in enumerate(np.arange(0,4.5,0.5)): tmp_df = lgbb_fill[lgbb_fill.beta==beta] for i,val in enumerate(tmp_df.alpha.unique()): tmp_df2 = tmp_df[tmp_df.alpha==val] ax[f_i].plot(tmp_df2[&#39;mach&#39;], tmp_df2[&#39;lift&#39;], label=&#39;alpha=&#39;+str(val.round(2))); # ax[beta].legend(bbox_to_anchor=[1,1]); ax[f_i].set_xlabel(&#39;mach&#39;);ax[f_i].set_ylabel(&#39;lift&#39;); ax[f_i].set_title(&#39;beta = &#39;+str(beta)) plt.tight_layout() . drag . fig, axs = plt.subplots(3,3,figsize=(15,10), sharey=True) ax = axs.ravel() for f_i,beta in enumerate(np.arange(0,4.5,0.5)): tmp_df = lgbb_fill[lgbb_fill.beta==beta] for i,val in enumerate(tmp_df.alpha.unique()): tmp_df2 = tmp_df[tmp_df.alpha==val] ax[f_i].plot(tmp_df2[&#39;mach&#39;], tmp_df2[&#39;drag&#39;], label=&#39;alpha=&#39;+str(val.round(2))); # ax[beta].legend(bbox_to_anchor=[1,1]); ax[f_i].set_xlabel(&#39;mach&#39;);ax[f_i].set_ylabel(&#39;drag&#39;); ax[f_i].set_title(&#39;beta = &#39;+str(beta)) plt.tight_layout() . pitch . fig, axs = plt.subplots(3,3,figsize=(15,10), sharey=True) ax = axs.ravel() for f_i,beta in enumerate(np.arange(0,4.5,0.5)): tmp_df = lgbb_fill[lgbb_fill.beta==beta] for i,val in enumerate(tmp_df.alpha.unique()): tmp_df2 = tmp_df[tmp_df.alpha==val] ax[f_i].plot(tmp_df2[&#39;mach&#39;], tmp_df2[&#39;pitch&#39;], label=&#39;alpha=&#39;+str(val.round(2))); # ax[beta].legend(bbox_to_anchor=[1,1]); ax[f_i].set_xlabel(&#39;mach&#39;);ax[f_i].set_ylabel(&#39;pitch&#39;); ax[f_i].set_title(&#39;beta = &#39;+str(beta)) plt.tight_layout() .",
            "url": "https://patel-zeel.github.io/Surrogates-GP-with-Python/gp/2021/02/22/Chap-2.html",
            "relUrl": "/gp/2021/02/22/Chap-2.html",
            "date": " • Feb 22, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Chapter 1",
            "content": "Content . import plotly.express as px import matplotlib.pyplot as plt from matplotlib.animation import FuncAnimation from matplotlib import rc import pandas as pd import numpy as np import scipy rc(&#39;font&#39;,size=12) . Simple order 1 polynomial: $ eta = 50 + 8x_1 + 3x_2$ . def first_order(x1, x2): return 50 + 8*x1 + 3*x2 fig, ax = plt.subplots() x1 = x2 = np.linspace(-1,1,100) X1, X2 = np.meshgrid(x1, x2) z = first_order(X1, X2) cntr = plt.contour(X1, X2, z, levels=20);plt.xlabel(&#39;X1&#39;);plt.ylabel(&#39;X2&#39;); plt.xticks([-1,-0.5,0,0.5,1]);plt.yticks([-1,-0.5,0,0.5,1]) ax.clabel(cntr) plt.colorbar(); . Adding interaction term: $ eta = 50 + 8x_1 + 3x_2 - 4x_1x_2$ . def first_order_i(x1, x2, c0=50, c1=8, c2=3, c3=4): return c0 + c1*x1 + c2*x2 - c3*x1*x2 x1 = x2 = np.linspace(-1,1,100) X1, X2 = np.meshgrid(x1, x2) fig, ax = plt.subplots() def update(c3): ax.cla() z = first_order_i(X1, X2, c3=c3) cntr = ax.contour(X1, X2, z, levels=20); ax.set_xlabel(&#39;X1&#39;);ax.set_ylabel(&#39;X2&#39;); ax.set_xticks([-1,-0.5,0,0.5,1]);ax.set_yticks([-1,-0.5,0,0.5,1]) ax.clabel(cntr); ax.set_title(&#39;c3 = &#39;+str(c3)) anim = FuncAnimation(fig, update, frames=np.arange(-4,5)) plt.close() rc(&#39;animation&#39;,html=&#39;jshtml&#39;) anim . &lt;/input&gt; Once Loop Reflect Adding squared terms: $ eta = 50 + 8x_1 + 3x_2 - 7x_1^2 - 3x_2^2 - 4x_1x_2$ . def simple_max(x1, x2, c0=50, c1=8, c2=3, c3=7, c4=3, c5=4): return c0 + c1*x1 + c2*x2 - c3*np.square(x1) - c4*np.square(x2) - c5*x1*x2 x1 = x2 = np.linspace(-1,1,100) X1, X2 = np.meshgrid(x1, x2) fig, ax = plt.subplots() def update(c3): ax.cla() z = simple_max(X1, X2, c3=c3) cntr = ax.contour(X1, X2, z, levels=20); ax.set_xlabel(&#39;X1&#39;);ax.set_ylabel(&#39;X2&#39;); ax.set_xticks([-1,-0.5,0,0.5,1]);ax.set_yticks([-1,-0.5,0,0.5,1]) ax.clabel(cntr); ax.set_title(&#39;c3 = &#39;+str(c3)) anim = FuncAnimation(fig, update, frames=np.arange(7-3,7+4)) plt.close() rc(&#39;animation&#39;,html=&#39;jshtml&#39;) anim . &lt;/input&gt; Once Loop Reflect Stationary ridge: $ eta = 80+4x_1+8x_2-3x_1^2-12x_2^2-12x_1x_2$ . def stat_ridge(x1, x2, c1=80, c2=4, c3=8, c4=3, c5=12, c6=12): return c1 + c2*x1 + c3*x2 - c4*np.square(x1) - c5*np.square(x2) - c6*x1*x2 x1 = x2 = np.linspace(-1,1,100) X1, X2 = np.meshgrid(x1, x2) fig, ax = plt.subplots() def update(c2): ax.cla() z = stat_ridge(X1, X2, c2=c2) cntr = ax.contour(X1, X2, z, levels=20); ax.set_xlabel(&#39;X1&#39;);ax.set_ylabel(&#39;X2&#39;); ax.set_xticks([-1,-0.5,0,0.5,1]);ax.set_yticks([-1,-0.5,0,0.5,1]) ax.clabel(cntr); ax.set_title(&#39;c2 = &#39;+str(c2)) anim = FuncAnimation(fig, update, frames=np.arange(-4,9)) plt.close() rc(&#39;animation&#39;,html=&#39;jshtml&#39;) anim . &lt;/input&gt; Once Loop Reflect Saddle point: $ eta = 80 + 4x_1 + 8x_2 - 2x_1 - 12x_2 - 12x_1x_2$ . def saddle(x1, x2, c0=80, c1=4, c2=8, c3=2, c4=12, c5=12): return c0 + c1*x1 + c2*x2 - c3*x1 - c4*x2 - c5*x1*x2 fig, ax = plt.subplots() x1 = x2 = np.linspace(-1,1,100) X1, X2 = np.meshgrid(x1, x2) z = saddle(X1, X2) cntr = plt.contour(X1, X2, z, levels=20);plt.xlabel(&#39;X1&#39;);plt.ylabel(&#39;X2&#39;); plt.xticks([-1,-0.5,0,0.5,1]);plt.yticks([-1,-0.5,0,0.5,1]) ax.clabel(cntr); . Aircraft wing weight example (1.2.1) . The weight of aircraft is considered a function of 9 parameters given by following equation, $$ W = 0.036 S_{ mathrm{w}}^{0.758} W_{ mathrm{fw}}^{0.0035} left( frac{A}{ cos^2 Lambda} right)^{0.6} q^{0.006} lambda^{0.04} left( frac{100 R_{ mathrm{tc}}}{ cos Lambda} right)^{-0.3} (N_{ mathrm{z}} W_{ mathrm{dg}})^{0.49} $$ . features = [&#39;Sw&#39;, &#39;Wfw&#39;, &#39;A&#39;, &#39;L&#39;, &#39;q&#39;, &#39;l&#39;, &#39;Rtc&#39;, &#39;Nz&#39;, &#39;Wdg&#39;] features_with_interaction = list(features) for f1_i in range(len(features)): for f2_i in range(f1_i+1, len(features)): features_with_interaction.append(features[f1_i]+&#39; &#39;+features[f2_i]) features_with_interaction = np.array(features_with_interaction) def wingwt(Sw=0.48, Wfw=0.4, A=0.38, L=0.5, q=0.62, l=0.344, Rtc=0.4, Nz=0.37, Wdg=0.38): ## put coded inputs back on natural scale Sw = Sw*(200 - 150) + 150 Wfw = Wfw*(300 - 220) + 220 A = A*(10 - 6) + 6 L = (L*(10 - (-10)) - 10) * np.pi/180 q = q*(45 - 16) + 16 l = l*(1 - 0.5) + 0.5 Rtc = Rtc*(0.18 - 0.08) + 0.08 Nz = Nz*(6 - 2.5) + 2.5 Wdg = Wdg*(2500 - 1700) + 1700 ## calculation on natural scale W = 0.036*Sw**0.758 * Wfw**0.0035 * (A/np.cos(L)**2)**0.6 * q**0.006 W = W * l**0.04 * (100*Rtc/np.cos(L))**(-0.3) * (Nz*Wdg)**(0.49) return(W) . Generating $100 times 100$ grid and checking interactions between $A$ (aspect ratio) and $N_z$ (ultimate load factor) keeping other parameters default. . x1 = x2 = np.linspace(0,1,100) A, Nz = np.meshgrid(x1, x2) wt = wingwt(A=A, Nz=Nz) fig, ax = plt.subplots() cntr = plt.contour(A, Nz, wt, levels=20);plt.xlabel(&#39;A&#39;);plt.ylabel(&#39;Nz&#39;); ticks = [0,0.2,0.4,0.6,0.8,1] plt.xticks(ticks);plt.yticks(ticks) ax.clabel(cntr); . Now, checking interactions between $ lambda$ (taper ratio) and $W_{fw}$ (weight of fuel in wing) in the same way. . x1 = x2 = np.linspace(0,1,100) l, Wfw = np.meshgrid(x1, x2) wt = wingwt(l=l, Wfw=Wfw) fig, ax = plt.subplots() cntr = plt.contour(l, Wfw, wt, levels=20);plt.xlabel(&#39;l&#39;);plt.ylabel(&#39;Wfw&#39;); ticks = [0,0.2,0.4,0.6,0.8,1] plt.xticks(ticks);plt.yticks(ticks) ax.clabel(cntr); . This interaction does not add a lot value in estimation of weight. . Generating grids for each pair (total $^9C_2$ = 36) and evaluating $100 times 100$ grid ($360K$ points) is not computationally tengible. We will use surrogate GP to learn the underlying phenomena using far lesser points. . Let&#39;s generate 1000 Latin Hypercube samples (LHS) for 9 dimensions of interest. . import pyDOE2 . X = pyDOE2.doe_lhs.lhs(9, 1000, random_state=42) plt.scatter(X[:,0], X[:,1], s=10); . Now, We will evaluate this input space in wingwt function to generate 1000 values for response variable weight. . Y = wingwt(*[X[:,i] for i in range(9)]) Y.shape . (1000,) . First we will try with 1st order polynomial regression with interaction terms. Let us use backward step selection, BIC criterion and 5 fold cv to select the best model. We will apply log transform over response variable y. . from sklearn.linear_model import LinearRegression from sklearn.preprocessing import PolynomialFeatures from sklearn.metrics import mean_squared_error from mlxtend.feature_selection import SequentialFeatureSelector as SFS trans = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False) sfs = SFS(estimator=LinearRegression(),k_features=1,forward=False,cv=0,n_jobs=-1,scoring=&#39;r2&#39;) X_trans = trans.fit_transform(X) Y_log = np.log(Y) sfs.fit(X_trans, Y_log); . Let&#39;s calculate BIC at each step and select best feature set. . def BIC(y_true, y_pred, k): # k - number of design variables RSS = np.square(y_true - y_pred).sum() n = y_true.shape[0] # Number of data points return k*np.log(n) + n*np.log(RSS/n) bic = [] f_subset = [] for f_num in range(1, len(features_with_interaction)): model = LinearRegression() f_idx = list(sfs.subsets_[f_num][&#39;feature_idx&#39;]) X_curr = X_trans[:, f_idx] model.fit(X_curr, Y_log) y_pred = model.predict(X_curr) bic.append(BIC(Y_log, y_pred, f_num)) f_subset.append(f_idx) plt.plot(bic); . best_f_subset = f_subset[np.argmin(bic)] print(&#39;Best subsets of features are&#39;, features_with_interaction[best_f_subset]) . Best subsets of features are [&#39;Sw&#39; &#39;A&#39; &#39;l&#39; &#39;Rtc&#39; &#39;Nz&#39; &#39;Wdg&#39; &#39;q Rtc&#39;] . model = LinearRegression() model.fit(X_trans[:, best_f_subset], Y_log) list(zip(features_with_interaction[best_f_subset], model.coef_)) . [(&#39;Sw&#39;, 0.21839627523361926), (&#39;A&#39;, 0.304927876180478), (&#39;l&#39;, 0.027836184996008453), (&#39;Rtc&#39;, -0.24454080966982894), (&#39;Nz&#39;, 0.41844378205613064), (&#39;Wdg&#39;, 0.19321447213716264), (&#39;q Rtc&#39;, 0.011651847094927881)] . Interaction between A and Nz is less likely to be captured though we know it exists. . Now, We will fit a GP. . import GPy . GP = GPy.models.GPRegression(X, Y.reshape(-1,1), GPy.kern.RBF(input_dim=9, active_dims=list(range(9)), ARD=True)) GP.optimize() . /home/patel_zeel/anaconda3/lib/python3.8/site-packages/GPy/kern/src/stationary.py:137: RuntimeWarning:overflow encountered in square /home/patel_zeel/anaconda3/lib/python3.8/site-packages/GPy/kern/src/stationary.py:138: RuntimeWarning:invalid value encountered in add . &lt;paramz.optimization.optimization.opt_lbfgsb at 0x7f9ba0482340&gt; . Creating a $100 times 100$ grid of A and Nz and setting other parameters to baseline. . params = [] for param in range(9): param_val = wingwt.__defaults__[param]*np.ones((10000,1)) params.append(param_val) # Modify A and Nz at positions 2 and 7 params[2] = A.reshape(-1,1) params[7] = Nz.reshape(-1,1) # Create test grid XX = np.hstack(params) XX.shape . (10000, 9) . pred_Y, pred_Var = GP.predict(XX) . fig, ax = plt.subplots(1,2, sharex=True, sharey=True,figsize=(10,4)) cntr1 = ax[0].contour(A, Nz, pred_Y.reshape(100,100), levels=20); ax[0].clabel(cntr1); wt = wingwt(A=A, Nz=Nz) cntr2 = ax[1].contour(A, Nz, wt, levels=20); ax[1].clabel(cntr2); ticks = [0,0.2,0.4,0.6,0.8,1] for i in range(2): ax[i].set_xlabel(&#39;A&#39;);ax[i].set_ylabel(&#39;Nz&#39;); ax[i].set_xticks(ticks);ax[i].set_yticks(ticks) ax[0].set_title(&#39;Predicted response&#39;) ax[1].set_title(&#39;Baseline response&#39;); . We have successfully captured the relationship between $N_z, A$ and $W$ with 1000 simulated points. . Now, doing 1D sensitivity analysis for all 9 process variables . preds = [] for i in range(9): print(i,end=&#39;&#39;) params = [] for param in range(9): param_val = wingwt.__defaults__[param]*np.ones((1000,1)) params.append(param_val) # Modify ith parameter params[i] = np.linspace(0,1,1000).reshape(-1,1) # Create test grid XX = np.hstack(params) preds.append(GP.predict(XX)[0]) print(&#39; Done&#39;) # wingwt . 012345678 Done . plt.figure(figsize=(15,5)) param_names = [&#39;Sw&#39;,&#39;Wfw&#39;,&#39;A&#39;,&#39;L&#39;,&#39;q&#39;,&#39;l&#39;,&#39;Rtc&#39;,&#39;Nz&#39;,&#39;Wdg&#39;] for i in range(9): plt.plot(np.linspace(0,1,1000), preds[i], label=param_names[i]) plt.legend(bbox_to_anchor=[1,1]); plt.xlabel(&#39;Process variable&#39;);plt.ylabel(&#39;Response variable&#39;); . Variables l, L, Wfw and q are not very useful in determining yields of response variable. . Homework . 1: Regression . The file wire.csv contains data relating the pull strength (pstren) of a wire bond (which we’ll treat as a response) to six characteristics which we shall treat as design variables: die height (dieh), post height (posth), loop height (looph), wire length (wlen), bond width on the die (diew), and bond width on the post (postw). (Derived from exercise 2.3 in Myers, Montgomery, and Anderson–Cook (2016) using data from Table E2.1.) . Write code that converts natural variables in the file to coded variables in the unit hypercube. Also, normalize responses to have a mean of zero and a range of 1. Use model selection techniques to select a parsimonious linear model for the coded data including, potentially, second-order and interaction effects. Use the fitted model to make a prediction for pull strength, when the explanatory variables take on the values c(6, 20, 30, 90, 2, 2), in the order above, with a full accounting of uncertainty. Make sure the predictive quantities are on the original scale of the data. . from sklearn.preprocessing import StandardScaler . wire_data = pd.read_csv(&#39;data/wire.csv&#39;) wire_data.head(1) . pstren dieh posth looph wlen diew postw . 0 8.0 | 5.2 | 19.6 | 29.6 | 94.9 | 2.1 | 2.3 | . Xscaler = StandardScaler() trans = PolynomialFeatures(degree=2, include_bias=False) X_trans = trans.fit_transform(wire_data.drop(&#39;pstren&#39;,axis=1)) X_scaled = Xscaler.fit_transform(X_trans) Yscaler = StandardScaler() y_scaled = Yscaler.fit_transform(wire_data[[&#39;pstren&#39;]]) . sfs = SFS(estimator=LinearRegression(), k_features=1, forward=False, cv=0, n_jobs=-1, scoring=&#39;r2&#39;) sfs.fit(X_scaled, y_scaled.squeeze()); . bic = [] f_subset = [] for f_num in range(1, X_scaled.shape[1]): model = LinearRegression() f_idx = list(sfs.subsets_[f_num][&#39;feature_idx&#39;]) X_curr = X_scaled[:, f_idx] model.fit(X_curr, y_scaled) y_pred = model.predict(X_curr) bic.append(BIC(y_scaled, y_pred, f_num)) f_subset.append(f_idx) plt.plot(bic); . Model selection is happening after including many features. Let&#39;s keep the best model. . best_f_subset = f_subset[np.argmin(bic)] . model = LinearRegression() model.fit(X_scaled[:, best_f_subset], y_scaled); . Now, we will predict at c(6, 20, 30, 90, 2, 2) . X_new = np.array([6, 20, 30, 90, 2, 2]).reshape(1,-1) X_new_trans = trans.transform(X_new) X_new_scaled = Xscaler.transform(X_new_trans) X_new_select = X_new_scaled[:, best_f_subset] print(&#39;pstren is&#39;, Yscaler.inverse_transform(model.predict(X_new_select))) . pstren is [[10.68053283]] . 2: Surrogates for sensitivity . Consider the so-called piston simulation function which was at one time a popular benchmark problem in the computer experiments literature. (That link, to Surjanovic and Bingham (2013)’s Virtual Library of Simulation Experiments (VLSE), provides references and further detail. VLSE is a great resource for test functions and computer simulation experiment data; there’s a page for the wing weight example as well.) Response $C(x)$ is the cycle time, in seconds, of the circular motion of a piston within a gas-filled cylinder, the configuration of which is described by seven-dimensional input vector $x=(M,S,V_0,k,P_0,T_a,T_0)$. $$ begin{aligned} C(x) = 2 pi sqrt{ frac{M}{k + S^2 frac{P_0 V_0}{T_0} frac{T_a}{V^2}}}, quad mbox{where } V &amp;= frac{S}{2k} left( sqrt{A^2 + 4k frac{P_0 V_0}{T_0} T_a} - A right) mbox{and } A &amp;= P_0 S + 19.62 M - frac{k V_0}{S} end{aligned} $$ . Table 1.2 describes the input coordinates of x, their ranges, and provides a baseline value derived from the middle of the specified range(s). . Explore $C(x)$ with techniques similar to those used on the wing weight example (§1.2.1). Start with a space-filling (LHS) design in 7d and fit a GP surrogate to the responses. Use predictive equations to explore main effects and interactions between pairs of inputs. In your solution, rather than showing all $^7C_2=21$ pairs, select one “interesting” and another “uninteresting” one and focus your presentation on those two. How do your surrogate predictions for those pairs compare to an exhaustive 2d grid-based evaluation and visualization of $C(x)$? . features = [&#39;M&#39;, &#39;S&#39;, &#39;V0&#39;, &#39;k&#39;, &#39;P0&#39;, &#39;Ta&#39;, &#39;T0&#39;] def piston(M=0.5, S=0.5, V0=0.5, k=0.5, P0=0.5, Ta=0.5, T0=0.5): ## put coded inputs back on natural scale M = M*(60 - 30) + 30 S = S*(0.02 - 0.005) + 0.005 V0 = V0*(0.01 - 0.002) + 0.002 k = k*(5000 - 1000) + 1000 P0 = P0*(110000 - 90000) + 90000 Ta = Ta*(296 - 290) + 290 T0 = T0*(360 - 340) + 340 A = P0*S + 19.62*M - k*V0/S Vfact1 = S/(2*k) Vfact2 = np.sqrt(np.square(A) + 4*k*(P0*V0/T0)*Ta) V = Vfact1 * (Vfact2 - A) fact1 = M fact2 = k + (np.square(S))*(P0*V0/T0)*(Ta/(np.square(V))) C = 2 * np.pi * np.sqrt(fact1/fact2) return(C) . Let&#39;s generate 1000 number of 7 dimensional LHS samples. . X_pist = pyDOE2.doe_lhs.lhs(7, samples=1000) Y_pist = piston(*[X_pist[:,i] for i in range(X_pist.shape[1])]) X_pist.shape, Y_pist.shape . ((1000, 7), (1000,)) . Now, we will fit a GP surrogate model to this data. . model = GPy.models.GPRegression(X_pist, Y_pist.reshape(-1,1), GPy.kern.RBF(input_dim=X_pist.shape[1], active_dims=list(range(X_pist.shape[1])), ARD=True)) model.optimize() . &lt;paramz.optimization.optimization.opt_lbfgsb at 0x7f9ba0493d30&gt; . Now we will do 1D sensitivity analysis and check how ground truth matches with surrogate. . preds = [] trues = [] for i in range(7): print(i,end=&#39;&#39;) params = [] for param in range(7): param_val = piston.__defaults__[param]*np.ones((1000,1)) params.append(param_val) # Modify ith parameter params[i] = np.linspace(0,1,1000).reshape(-1,1) # Create test grid XX = np.hstack(params) preds.append(model.predict(XX)[0]) trues.append(piston(*[XX[:,i] for i in range(7)])) print(&#39; Done&#39;) . 0123456 Done . fig, ax = plt.subplots(1,2,figsize=(16,5), sharex=True, sharey=True) param_names = [&#39;M&#39;,&#39;S&#39;,&#39;V0&#39;,&#39;k&#39;,&#39;P0&#39;,&#39;Ta&#39;,&#39;T0&#39;] linestyles = [&#39;--&#39;,&#39;-&#39;] for i in range(7): ax[0].plot(np.linspace(0,1,1000), preds[i], linestyles[i%2],label=param_names[i]) ax[1].plot(np.linspace(0,1,1000), trues[i], linestyles[i%2],label=param_names[i]) ax[0].set_xlabel(&#39;Process variable&#39;);ax[1].set_xlabel(&#39;Process variable&#39;); ax[0].set_title(&#39;Main effects - Surrogate&#39;);ax[1].set_title(&#39;Main effects - Ground truth&#39;) ax[0].set_ylabel(&#39;Cycle speed (in seconds)&#39;); ax[1].legend(bbox_to_anchor=[1,1]); . We see that S, V0, M and k are causing major impact on cycle speed of the piston. Let&#39;s visualize relationship between S and M0 that should be &quot;interesting&quot; (as asked in question). . X_all = np.ones((10000,7))*0.5 S, V0 = np.meshgrid(np.linspace(0,1,100), np.linspace(0,1,100)) X_all[:,1] = S.ravel() X_all[:,2] = V0.ravel() C_true = piston(*[X_all[:,i] for i in range(X_all.shape[1])]) C_pred = model.predict(X_all)[0] fig, ax = plt.subplots(1,2, sharex=True, sharey=True,figsize=(10,4)) cntr1 = ax[0].contour(S, V0, C_pred.reshape(100,100), levels=20); ax[0].clabel(cntr1); cntr2 = ax[1].contour(S, V0, C_true.reshape(100,100), levels=20); ax[1].clabel(cntr2); ticks = [0,0.2,0.4,0.6,0.8,1] for i in range(2): ax[i].set_xlabel(&#39;S&#39;);ax[0].set_ylabel(&#39;V0&#39;); ax[i].set_xticks(ticks);ax[i].set_yticks(ticks) ax[0].set_title(&#39;Predicted response&#39;) ax[1].set_title(&#39;Baseline response&#39;); . Now, checking relationship between Ta and P0. . X_all = np.ones((10000,7))*0.5 Ta, P0 = np.meshgrid(np.linspace(0,1,100), np.linspace(0,1,100)) X_all[:,5] = Ta.ravel() X_all[:,4] = P0.ravel() C_true = piston(*[X_all[:,i] for i in range(X_all.shape[1])]) C_pred = model.predict(X_all)[0] fig, ax = plt.subplots(1,2, sharex=True, sharey=True,figsize=(10,4)) cntr1 = ax[0].contour(Ta, P0, C_pred.reshape(100,100), levels=20); ax[0].clabel(cntr1); cntr2 = ax[1].contour(Ta, P0, C_true.reshape(100,100), levels=20); ax[1].clabel(cntr2); ticks = [0,0.2,0.4,0.6,0.8,1] for i in range(2): ax[i].set_xlabel(&#39;Ta&#39;);ax[0].set_ylabel(&#39;P0&#39;); ax[i].set_xticks(ticks);ax[i].set_yticks(ticks) ax[0].set_title(&#39;Predicted response&#39;) ax[1].set_title(&#39;Baseline response&#39;); . As we can see the relationship is &quot;uninteresting&quot; both marginally and jointly. . 3: Optimization . Consider two-dimensional functions $f$ and $c$ , defined over $[ 0 , 1 ]^2$ ; $f$ is a re-scaled version of the so-called Goldstein–Price function, and is defined in terms of auxiliary functions $a$ and $b$. $$ begin{aligned} f(x) &amp;= frac{ log left[(1+a(x)) (30 + b(x)) right] - 8.69}{2.43} quad mbox{with} a(x) &amp;= left(4 x_1 + 4 x_2 - 3 right)^2 &amp; ; ; ; times left[ 75 - 56 left(x_1 + x_2 right) + 3 left(4 x_1 - 2 right)^2 + 6 left(4 x_1 - 2 right) left(4 x_2 - 2 right) + 3 left(4 x_2 - 2 right)^2 right] b(x) &amp;= left(8 x_1 - 12 x_2 +2 right)^2 &amp; ; ; ; times left[-14 - 128 x_1 + 12 left(4 x_1 - 2 right)^2 + 192 x_2 - 36 left(4 x_1 - 2 right) left(4 x_2 - 2 right) + 27 left(4 x_2 - 2 right)^2 right] end{aligned} $$ Separately, let a “constraint” function $c$ be defined as, $$ c(x) = frac{3}{2} - x_1 - 2x_2 - frac{1}{2} sin(2 pi(x_1^2 - 2x_2)) $$ . Evaluate $f$ on a grid and make an image and/or image–contour plot of the surface. | Use a library routine (e.g., optim in R) to find the global minimum. When optimizing, pretend you don’t know the form of the function; i.e., treat it as a “blackbox”. Initialize your search randomly and comment on the behavior over repeated random restarts. About how many evaluations does it take to find the local optimum in each initialization repeat; about how many to reliably find the global one across repeats? | Now, re-create your plot from #a with contours only (no image), and then add color to the plot indicating the region(s) where $c(x)&gt;0$ and $c(x)≤0$ ,respectively. To keep it simple, choose white for the latter, say. | Use a library routine (e.g., nloptr in R) to solve the following constrained optimization problem: min $f(x)$ such that $c(x)≤0$ and $x ∈ [ 0 , 1 ]^2$. Initialize your search randomly and comment on the behavior over repeated random restarts. About how many evaluations does it take to find the local valid optimum in each initialization repeat; about how many to reliably find the global one across repeats? | Ans | . First, let&#39;s create a grid and make an image of the surface of $f$. . def goldprice(xx): x1bar = 4*xx[0] - 2 x2bar = 4*xx[1] - 2 fact1a = np.square(x1bar + x2bar + 1) fact1b = 19 - 14*x1bar + 3*np.square(x1bar) - 14*x2bar + 6*x1bar*x2bar + 3*np.square(x2bar) fact1 = 1 + fact1a*fact1b fact2a = np.square(2*x1bar - 3*x2bar) fact2b = 18 - 32*x1bar + 12*np.square(x1bar) + 48*x2bar - 36*x1bar*x2bar + 27*np.square(x2bar) fact2 = 30 + fact2a*fact2b prod = fact1*fact2 y = (np.log(prod) - 8.693) / 2.427 return y def c(xx): x1, x2 = xx return 1.5 - x1 - 2*x2 - 0.5*np.sin(2*np.pi*(np.square(x1)-2*x2)) . x1, x2 = np.meshgrid(np.linspace(0,1,100), np.linspace(0,1,100)) gprice = goldprice([x1, x2]) cisneg = c([x1, x2])&lt;=0 fig, ax = plt.subplots(figsize=(15,6)) cntr = plt.contour(x1, x2, gprice, levels=20);plt.xlabel(&#39;x1&#39;);plt.ylabel(&#39;x2&#39;); cntr2 = plt.contourf(x1, x2, cisneg, alpha=0.4) ax.clabel(cntr) plt.colorbar(); plt.title(&#39;goldstain function shown by contour lines. Yellow shade is $c(x) &lt;= 0$&#39;); . Optimizing without constaints . optims = [] N = 200 for seed in range(N): np.random.seed(seed) res = scipy.optimize.minimize(goldprice, [np.abs(np.random.rand()),np.abs(np.random.rand())]) optims.append(res.fun) plt.figure(figsize=(15,4)) plt.xticks(list(range(N))[::10]) plt.scatter(range(N),optims);plt.xlabel(&#39;Seed&#39;);plt.ylabel(&#39;Optimal function value&#39;); goptim = np.min(optims) print(&#39;Out of&#39;,N,&#39;restarts&#39;,(np.array(optims)&lt;goptim+0.01).sum(),&#39;found global minima&#39;,goptim) . Out of 200 restarts 104 found global minima -3.129125550610589 . Optimizing with constaints, $c(x) le 0$ . optims = [] for seed in range(N): np.random.seed(seed) cons = scipy.optimize.NonlinearConstraint(c, -np.inf, 0) res = scipy.optimize.minimize(goldprice, [np.abs(np.random.rand()),np.abs(np.random.rand())], constraints=cons) optims.append(res.fun) plt.figure(figsize=(15,4)) plt.xticks(list(range(N))[::10]) plt.scatter(range(N),optims);plt.xlabel(&#39;Seed&#39;);plt.ylabel(&#39;Optimal function value&#39;); goptim = np.min(optims) plt.ylim(-3,4); print(&#39;Out of&#39;,N,&#39;restarts&#39;,(np.array(optims)&lt;goptim+0.01).sum(),&#39;found global minima&#39;,goptim) . Out of 200 restarts 70 found global minima -2.180388388271195 . Optimizing with constaints, $c(x) &gt; 0$ . optims = [] for seed in range(N): np.random.seed(seed) cons = scipy.optimize.NonlinearConstraint(c, 0, np.inf) res = scipy.optimize.minimize(goldprice, [np.abs(np.random.rand()),np.abs(np.random.rand())], constraints=cons) optims.append(res.fun) plt.figure(figsize=(15,4)) plt.xticks(list(range(N))[::10]) plt.scatter(range(N),optims);plt.xlabel(&#39;Seed&#39;);plt.ylabel(&#39;Optimal function value&#39;); goptim = np.min(optims) plt.ylim(-4,4); print(&#39;Out of&#39;,N,&#39;restarts&#39;,(np.array(optims)&lt;goptim+0.01).sum(),&#39;found global minima&#39;,goptim) . Out of 200 restarts 118 found global minima -3.1291255506097215 .",
            "url": "https://patel-zeel.github.io/Surrogates-GP-with-Python/gp/2021/02/22/Chap-1.html",
            "relUrl": "/gp/2021/02/22/Chap-1.html",
            "date": " • Feb 22, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://patel-zeel.github.io/Surrogates-GP-with-Python/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://patel-zeel.github.io/Surrogates-GP-with-Python/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}